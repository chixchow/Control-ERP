---
phase: 03-wiki-knowledge-formalization
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - output/wiki/VERIFICATION.md
autonomous: true

must_haves:
  truths:
    - "Wiki crawl completeness is provably documented -- 1,769 crawl state entries accounted for with namespace deduplication explained"
    - "All 12 knowledge extracts are verified to contain actionable, domain-specific content (not raw page dumps)"
    - "Discrepancy between '6 required' and '12 actual' extracts is reconciled with clear documentation"
  artifacts:
    - path: "output/wiki/VERIFICATION.md"
      provides: "Crawl completeness proof and extract quality audit"
      min_lines: 100
      contains: "1,769"
  key_links:
    - from: "output/wiki/VERIFICATION.md"
      to: "output/wiki/_crawl_state.json"
      via: "Crawl state entry count verification"
      pattern: "1,769.*downloaded"
    - from: "output/wiki/VERIFICATION.md"
      to: "output/wiki/extracts/"
      via: "Extract quality audit per file"
      pattern: "database_integration|orders_accounting|production_inventory|crm_payroll|sql_queries|howto_troubleshooting"
---

<objective>
Verify wiki crawl completeness (WIKI-01) and audit all knowledge extract quality (WIKI-02).

Purpose: Downstream phases and skills rely on the wiki knowledge base being complete and accurate. This plan proves completeness with hard numbers and audits extract quality so formalization work in Plan 02 can proceed on verified material.

Output: `output/wiki/VERIFICATION.md` -- a single document proving crawl completeness and extract quality.
</objective>

<execution_context>
@/Users/cain/.claude/get-shit-done/workflows/execute-plan.md
@/Users/cain/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-wiki-knowledge-formalization/03-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Verify wiki crawl completeness</name>
  <files>output/wiki/VERIFICATION.md</files>
  <action>
Write a Python verification script (in-memory, no file creation needed) that:
1. Loads `output/wiki/_crawl_state.json` and counts entries by status and subdomain
2. Counts actual files on disk in `output/wiki/control/`, `output/wiki/reports/`, `output/wiki/support/`
3. Identifies the 22 `pages:` namespace duplicates in reports/ subdomain
4. Produces a verification table

Then write the first section of `output/wiki/VERIFICATION.md` with:
- **Crawl Summary**: Total entries (1,769), breakdown by subdomain (control: 1,146, reports: 46, support: 577)
- **Disk File Reconciliation**: Actual file counts (control: 1,146, reports: 24, support: 577), total unique: 1,747
- **Namespace Deduplication**: Explanation that 22 `pages:` entries in reports/ are DokuWiki namespace aliases pointing to same content
- **Verdict**: Crawl is COMPLETE. All 1,769 URLs visited. 1,747 unique content pages stored.
- **Existing Index Assets**: Note KNOWLEDGE_BASE.md (282 lines), INDEX.md (~4000 lines), CATALOG.md (18,667 lines, 16 categories), TOPICS.json (23,455 lines)

Do NOT re-run the crawl. Do NOT modify existing wiki files. This is a verification audit.
  </action>
  <verify>
Run: `python3 -c "import json; d=json.load(open('output/wiki/_crawl_state.json')); print('downloaded:', len(d['downloaded']))"` to confirm entry count (expected: 1,769). The crawl state JSON has structure `{"downloaded": {"url": status, ...}, ...}`.
Verify `output/wiki/VERIFICATION.md` exists and contains "1,769" and "COMPLETE".
  </verify>
  <done>VERIFICATION.md contains a crawl completeness section with exact counts, namespace deduplication explanation, and a COMPLETE verdict matching all three subdomains.</done>
</task>

<task type="auto">
  <name>Task 2: Audit all 12 knowledge extracts for quality</name>
  <files>output/wiki/VERIFICATION.md</files>
  <action>
For each of the 12 extract files in `output/wiki/extracts/`, read the first ~100 lines and spot-check content to assess:
1. **Structure**: Does it have clear section headers, not just raw text?
2. **Actionable content**: Does it contain specific table names, SQL examples, field values, or procedure details (not just prose descriptions)?
3. **Domain coverage**: Does it actually cover what its filename claims?
4. **Size**: Line count and approximate KB size

The 12 extracts to audit:
- database_integration_knowledge.md (original 6)
- orders_accounting_knowledge.md (original 6)
- production_inventory_knowledge.md (original 6)
- crm_payroll_system_knowledge.md (original 6)
- sql_queries_reference.md (original 6)
- howto_troubleshooting_knowledge.md (original 6)
- cfl_formula_language_knowledge.md (additional)
- macros_automation_knowledge.md (additional)
- parts_knowledge.md (additional)
- products_pricing_knowledge.md (additional)
- udfs_custom_fields_knowledge.md (additional)
- email_notifications_knowledge.md (additional)

Append to `output/wiki/VERIFICATION.md` an **Extract Quality Audit** section with:
- Table: Extract name | Lines | Size | Domain Coverage | Quality Rating (GOOD/ADEQUATE/NEEDS-WORK)
- For each extract, 1-2 sentence assessment of content quality
- **Reconciliation note**: "WIKI-02 requires 6 extracts. All 6 original extracts exist and are verified. 6 additional extracts were produced in a second pass, exceeding the requirement."
- **Overall verdict**: Whether extracts collectively provide actionable reference material for downstream skills

Quality rating criteria:
- GOOD: Structured sections, specific SQL/table references, actionable examples
- ADEQUATE: Structured but less specific, still useful reference
- NEEDS-WORK: Raw dumps, missing key information, unstructured

Note: Based on research, all 12 extracts total 13,413 lines / ~519 KB. The research already confirmed they are substantive. This audit formalizes that finding.
  </action>
  <verify>
Verify `output/wiki/VERIFICATION.md` contains an "Extract Quality Audit" section with all 12 extract names listed.
Verify the file contains a reconciliation note about "6 required" vs "12 actual".
  </verify>
  <done>VERIFICATION.md contains crawl completeness proof (Task 1) AND extract quality audit (Task 2) with per-extract assessments, reconciliation of the 6 vs 12 discrepancy, and overall quality verdict.</done>
</task>

</tasks>

<verification>
- `output/wiki/VERIFICATION.md` exists with both sections (crawl completeness + extract audit)
- Crawl section has exact numbers: 1,769 total, 1,146 + 46 + 577 by subdomain, 1,747 unique on disk
- Extract section lists all 12 files with quality ratings
- Document has clear COMPLETE/PASS verdicts for WIKI-01 and WIKI-02
</verification>

<success_criteria>
- WIKI-01 is provably satisfied: 1,769 pages crawled, completeness documented with namespace deduplication explained
- WIKI-02 is provably satisfied: All 6 required extracts exist and are quality-verified, plus 6 additional extracts
- A future reader of VERIFICATION.md can understand the crawl scope, file counts, and extract quality without running any scripts
</success_criteria>

<output>
After completion, create `.planning/phases/03-wiki-knowledge-formalization/03-01-SUMMARY.md`
</output>
